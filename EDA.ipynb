{"cells":[{"source":["import numpy as np\n","import pandas as pd\n","import spacy\n","from pathlib import Path\n","import random\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\"\"\"First we would like to run spacy NLP on each of the documents\n","that we have randomly sampled from the larger dataset. Because they\n","have been sampled at random, we expect that they will be representative\n","of the entire dataset.\n","\"\"\"\n","from spacyOps import edaLabeler\n","nlp = spacy.load('en')\n","\n","# Add sentancizer and custom labeler\n","sentencizer = nlp.create_pipe('sentencizer')\n","nlp.add_pipe(sentencizer)\n","nlp.add_pipe(edaLabeler)\n","\n","wikiDataPath = Path('./Wiki-sample')\n","wikiFiles = [x for x in wikiDataPath.glob('**/*.txt') if x.is_file()]"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print('There are {} files in the wiki directory'.format(len(files)))\n","print('Running SpaCy NLP on the files (this will take a few minutes)')\n","wikiDocs = [nlp(fo.read_text(encoding='utf-8')) for fo in wikiFiles]"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","podcastDataPath = Path('./data/podcasts')\n","podcastFiles = [x for x in podcastDataPath.glob('**/*.txt') if x.is_file()]\n","print('There are {} files in the podcast directory'.format(len(podcastFiles)))\n","print('Running SpaCy NLP on the files (this will take a few minutes)')\n","\n","podcastDocs = [nlp(fo.read_text(encoding='utf-8')) for fo in podcastFiles]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\"\"\"First let's provide some stucture by organizing our documents into a\n","pandas dataframe\"\"\""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","wiki_df = pd.DataFrame()\n","wiki_df['doc_name'] = [str(x.stem) for x in wikiFiles]\n","print(wiki_df.head())\n","\n","pod_df = pd.DataFrame()\n","pod_df['doc_name'] = [str(x.stem) for x in podcastFiles]\n","print(pod_df.head())\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Next let's compare some key characteristics about the 2 datasets\n","\n","# Number of words - Right away, we can see that the podcasts are much longer\n","# on average than the wiki articles."],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["wiki_num_words = [len(x) for x in docs]\n","wiki_df['num_words'] = wiki_num_words\n","\n","pod_num_words = [len(x) for x in podcastDocs]\n","pod_df['num_words'] = pod_num_words"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Let's add our spacy tokenized sentences and corresponding labels to the dataframe\n","# number of sentences\n","label_arr = []\n","sent_arr = []\n","for doc in docs:\n","    labels = np.array(doc.user_data['labels'])\n","    sents = np.array(doc.user_data['sents'], dtype=object)\n","    label_arr.append(labels)\n","    sent_arr.append(sents)\n","\n","wiki_df['sents'] = sent_arr\n","wiki_df['labels'] = label_arr\n","\n","label_arr = []\n","sent_arr = []\n","for doc in podcastDocs:\n","    labels = np.array(doc.user_data['labels'])\n","    sents = np.array(doc.user_data['sents'], dtype=object)\n","    label_arr.append(labels)\n","    sent_arr.append(sents)\n","\n","pod_df['sents'] = sent_arr\n","pod_df['labels'] = label_arr"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","wiki_df['num_sent'] = wiki_df.apply(lambda row: len(row.sents), axis=1)\n","pod_df['num_sent'] = pod_df.apply(lambda row: len(row.sents), axis=1)\n","\n","sns.distplot( wiki_df['num_sent'] , color=\"skyblue\", label='Wiki')\n","sns.distplot( pod_df['num_sent'] , color=\"red\", label='Podcast')\n","plt.legend()\n","plt.show()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# number of segments\n","wiki_df['num_seg'] = wiki_df.apply(lambda row: sum(row.labels), axis=1)\n","pod_df['num_seg'] = pod_df.apply(lambda row: sum(row.labels), axis=1)\n","\n","sns.distplot( wiki_df['num_seg'] , color=\"skyblue\", label='Wiki')\n","sns.distplot( pod_df['num_seg'] , color=\"red\", label='Podcast')\n","plt.legend()\n","plt.show()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# average segment length\n","wiki_df['avg_seg_len'] = wiki_df.apply(lambda row: row.num_sent/row.num_seg, axis=1)\n","pod_df['avg_seg_len'] = pod_df.apply(lambda row: row.num_sent/row.num_seg, axis=1)\n","\n","sns.distplot( wiki_df['avg_seg_len'] , color=\"skyblue\", label='Wiki')\n","sns.distplot( pod_df['avg_seg_len'] , color=\"red\", label='Podcast')\n","plt.legend()\n","plt.show()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# parts of speech"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# poloarity"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\"\"\"One of the major challenges we would like to address is whether our neural network\n","can be trained on the Wiki dataset, but also generalize to the podcast data. Central\n","to this question is how similar the datasets are. Wikipedia articles are typically\n","dispassionate, informative, and well structured. Conversely, podcasts are conversational\n","in nature. They assume all of the nuances of human speech such as studders, runaway thoughts,\n","interruptions, etc.\"\"\"\n","\n","# Actionable insights - From this exploratory data analysis, we can use some of the\n","# insights gained to inform the construction of our network and preprocessing decisions.\n","# Specifically, we note the large discrepency in average segment length between the podcasts\n","# and wiki datasets. To combat this effect, we decided to eliminate segments shorter than 5\n","# sentences in the wiki dataset during preprocessing. Additionally, we eliminated all Wiki \n","# articles with fewer than 3 segments. Our goal is to extract from the wiki dataset articles\n","# that will be more structurally similar to the podcasts. "],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}